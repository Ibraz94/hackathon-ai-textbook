{
  "id": "modules/vla",
  "title": "Module 4: Vision-Language-Action (VLA)",
  "description": "This module explores the cutting-edge field of Vision-Language-Action (VLA), where robots integrate visual perception with natural language understanding to perform complex actions. We will cover how Large Language Models (LLMs) are being integrated into robotic systems to enable more intuitive and intelligent human-robot interaction and multimodal perception.",
  "source": "@site/docs/modules/vla.mdx",
  "sourceDirName": "modules",
  "slug": "/modules/vla",
  "permalink": "/physical-ai-textbook/docs/next/modules/vla",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/vla.mdx",
  "tags": [],
  "version": "current",
  "sidebarPosition": 4,
  "frontMatter": {
    "sidebar_position": 4,
    "title": "Module 4: Vision-Language-Action (VLA)"
  },
  "sidebar": "modulesSidebar",
  "previous": {
    "title": "Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)",
    "permalink": "/physical-ai-textbook/docs/next/modules/ai-robot-brain"
  },
  "next": {
    "title": "Hardware Context",
    "permalink": "/physical-ai-textbook/docs/next/hardware-context"
  }
}